/**
 * 浏览器
 * 
 * 1.能不能说一下浏览器缓存
 * 
 * 缓存是性能优化中非常重要的一环，浏览器的缓存机制对开发也是非常重要的知识点，接下来以三个部分把浏览器的
 * 缓存机制说清楚。
 * 
 * ·强缓存
 * ·协商缓存
 * ·缓存位置
 * 
 * ·强缓存
 * 浏览器中的缓存作用分为两种情况，一种是需要发送HTTP请求，一种是不需要发送
 * 
 * 首先检查强缓存，这个阶段不需要发送HTTP请求
 * 
 * 如何来检查呢？ 通过相应的字段来进行，但是说起这个字段就有点门道了
 * 
 * 在HTTP/1.0和HTTP/1.1当中，这个字段是不一样的。在早期，也就是HTTP/1.0时期使用的Expires，
 * 而HTTP/1.1使用的Cache-Control,让我们首先来看看Expires
 * 
 * Expires
 * 
 * Expires即过期时间，存在于服务端返回的响应头中，告诉浏览器在这个过期时间之前可以直接从缓存里获取数据，
 * 无需再次请求。比如下面
 * Expires：wed, 22 Nov 2019 08:41:00 GMT
 * 
 * 表示资源在2019年11月22日8点41分过期，过期了就得向服务器发送请求
 * 
 * 这个方式看上去没用什么问题，合情合理，但其实潜藏了一个坑，那就是服务器的时间和浏览器的时间可能并不一致
 * 那服务器返回的这个过期时间可能就是不准确的。因此这种方式很快的后来HTTP/1.1版本中被舍弃了。
 * 
 * Cache-Control
 * 
 * 在HTTP/1.1中，采用一个非常关键的字段：Cache-Control这个字段也是存在于服务端返回的响应头中，
 * 它和Expires本质的不同在于它并没有采用具体的过期时间点这个方式，而是采用过期时长来控制缓存，对应的字段
 * 是max-age。比如这个例子：
 * Cache-Control: max-age=3600
 * 代表这个响应返回后3600秒，也就是一个小时内可以直接使用缓存
 * 
 * 如果你觉得它只有max-age一个属性的话，就打错特错了。
 * 
 * 它其实可以组合非常多的指令，完成更多场景的缓存判断，将一些关键的属性列举如下
 * 
 * public：客户端和代理服务器都可以缓存。
 * 因为一个请求可能要经过不同对的代理服务器最后才达到目标服务器，那么结果就是不仅仅浏览器可以缓存数据，
 * 中间的任何代理节点都可以进行缓存。
 * 
 * private：这种情况就是只有浏览器能缓存了，中间代理服务器不能缓存。
 * 
 * no-cache: 跳过当前的强缓存，发送HTTP请求，即进入协商缓存阶段。
 * 
 * no-store: 非常粗暴，不进行任何形式的缓存
 * 
 * s-maxage: 这个和max-age长的比较像，但是区别在于s-maxage是针对代理服务器的缓存时间。
 * 
 * 值得注意的是，当Expires和Cache-Control同时存在的时候，Cache-Control会优先考虑
 * 
 * 当然，还存在一种情况，当资源缓存时间超时了，也就是强缓存失效了，接下来怎么办？
 * 
 * 没错，这样就进入到了第二级屏障---协商缓存 了
 * 
 * 
 * ·协商缓存
 * 
 * 强缓存失效之后，浏览器在请求头中携带相应的缓存tag来向服务器发请求，由浏览器跟这个tag，来决定是否使用缓存
 * 这就是协商缓存
 * 
 * 具体来说，这样的缓存tag分为两种：Last-Modified和ETag，两者各有优势，并不存在谁对谁有绝对优势，
 * 跟上面强缓存的两个Tag不一样
 * 
 * Last-Modified
 * 
 * 即最后修改时间。在浏览器第一次给服务器发送请求后，服务器会在响应头中加这个字段。
 * 
 * 浏览器接收到后，如果再次请求，会在请求头中携带If-Modified-Since字段，这个字段的值也就是服务器传来的最后修改时间
 * 
 * 服务器拿到请求头中的If-Modified-Since的字段后，其实会和这个服务器中 该资源的最后修改时间对比
 * ·如果请求头中的这个值小于最后修改时间，说明时时候更新了。返回新的资源，跟常规的HTTP请求响应的流程一样。
 * ·否则返回304，告诉浏览器直接用缓存
 * 
 * ETag
 * 
 * ETag是服务器根据当前文件的内容，给文件生成唯一标识，只要里面的内容有变动，这个值就会变。服务器通过响应头把这个
 * 值给浏览器
 * 
 * 浏览器接收到ETag的值，会在下次请求时，将这个值作为If-None-Match这个字段的内容，并放到请求头中，然后发给服务器
 * 
 * 服务器接收到If-None-Match后，会跟服务器上该资源的ETag进行对比：
 * ·如果两者不一样，说明要更新了。返回新的资源，跟常规的HTTP请求流程一样
 * ·否则返回304，告诉浏览器直接用缓存
 * 
 * 两者相比
 * 1.在精准度上， ETag优于Last-Modified。由于ETag是按照内容给资源上标识，因此能准确感知资源变化。
 * 而Last-Modified就不一样了，它在一些特殊的情况并不能准确感知资源的变化，主要有两种情况：
 * ·编辑了资源文件，但是文件内容并没有改变，这样也会造成缓存失效。
 * Last-Modified能够感知的单位时间是秒，如果文件在一秒内改变了多次，那么这个时候的Last-Modified并没有体现出修改了
 * ·在性能上，Last-Modified优于ETag，也很简单理解，Last-Modified仅仅只是记录一个时间点，而ETag需要根据文件的具体
 * 内容生成哈希值
 * 
 * 另外，如果两种方式都支持的话，服务器会优先考虑ETag
 * 
 * 缓存位置
 * 
 * 前面我们已经提到，当强缓存命中或者协商缓存中服务器返回304的时候，我们直接从缓存中获取资源。那么这些资源究竟
 * 缓存在什么位置呢
 * 
 * 浏览器中的缓存位置一共有四种，按优先级从高到低排列分别是：
 * 
 * Service Worker
 * Memory Cache
 * Disk Cache
 * Push Cache
 * 
 * Service Worker
 * 
 * Service Worker借鉴了Web Worker的思路，即让JS运行在主线程之外，由于他脱离了浏览器的窗口，因此无法直接访问DOM
 * 虽然如此，但它仍能帮助我们完成很多有用的功能，比如离线缓存、消息推送和网络代理等功能。其中的离线缓存就是
 * Service Worker Cache
 * 
 * Service Worker同时也是PWA的重要实现机制，关于它的细节和特性，我们将在后面的PWA中分享
 * 
 * Memory Cache和Disk Cache
 * 
 * Memory Cache是指的是内存缓存，从效率上讲它是最快的。但是从存活时间上来讲又是最端的，当渲染进程结束后，
 * 内存缓存也就不存在了。
 * 
 * Disk Cache就是存储在磁盘中的缓存，从存取效率上讲是比内存缓存慢的，但是它的优势在于存储容量和存储时长，
 * 稍微有计算机基础的应该好理解
 * 
 * 两者的优劣，那浏览器如何决定将资源放进内存还是硬盘呢？主要策略如下：
 * 比较大的JS,CSS文件会被直接丢进磁盘，反之丢进内存
 * 内存使用率比较高时，文件优先进内存
 * 
 * Push Cache
 * 即推送缓存，这是浏览器缓存的最后一道防线。它是HTTP/2中的内容，虽然现在应用的并不广泛，但随着HTTP/2的
 * 推广，它的应用越来越广泛。
 * 
 * 总结：
 * 
 * 对浏览器的缓存机制来做一个简单总结
 * 
 * 首先通过Cache-Control验证强缓存是否可用
 * 
 * ·如果强缓存直接可用，直接使用
 * ·否则进入协商缓存，即发送HTTP请求，服务器通过请求头中的If-Modified-Since或者If-None-Match字段检查资源是否更新
 * ·  ·若资源更新，返回资源和200状态码
 * ·  ·否则，返回304，告诉浏览器直接从缓存获取资源
 * 
 */

// const http = require('http')
// const fs = require('fs')

// http.createServer((req, res) => {
  
//   if(req.url.indexOf('a.css') >= 0){
//     var stat1 = fs.statSync('./a.css');
//     // let ifModifiedSince =req.headers['if-modified-since']//获取
//     // if(new Date(ifModifiedSince).getTime()+1000 > new Date(stat1.mtime).getTime()){
//     //     res.statusCode = 304
//     //     return res.end()//直接返回
//     // }

//     // let ifNoneMatch =req.headers['if-none-match']
//     // console.log(ifNoneMatch, stat1.size, ifNoneMatch == stat1.size)
//     // if(ifNoneMatch == stat1.size){
//     //   res.statusCode = 304
//     //   return res.end()//直接返回
//     // }
    
//     fs.readFile('./a.css', (err, html) => {
//       res.writeHead(200, {
//         'Content-Type': 'text/css;charset=UTF-8',
//         "Cache-Control":"max-age=5",
//         "ETag" : stat1.size
//         //'Last-Modified': stat1.mtime
//       });
//       res.end(html)
//     })
//   }else if(req.url.indexOf('aaa') >= 0){
//     var stat = fs.statSync('./index.html');
//     fs.readFile('./index.html', (err, html) => {
//       res.writeHead(200, {
//         'Content-Type': 'text/html',
//         "Cache-Control":"max-age=5",
//         'Expires': new Date(new Date().getTime() + 1000),
//         'Last-Modified': stat.mtime
//       });
//       res.end(html)
//     })
//   }else{
//     res.end('xxx');
//   }
// }).listen(8888)


/**
 * 浏览器的本地存储和各自优劣
 * 
 * 浏览器的本地存储主要分为Cookie，WebStorage和IndexDB，其中WebStorage又可用分为localStorage和sessionStorage
 * 接下来我们就一一分析这些本地存储方式
 * 
 * Cookie
 * 
 * Cookie最开始被设计出来并不是用来做本地存储的，而是为了弥补HTTP在状态管理上的不足
 * 
 * HTTP协议是一个无状态协议，客户端向服务端发请求，服务器返回响应，故事就这样结束了，但是下次发请求
 * 如何让服务端知道客户端是谁呢？
 * 
 * 这种背景下，就产生了cookie
 * 
 * Cookie本质就是浏览器里面存储的一个很小的文本文件，内部以键值对的方式来存储（在chrome开发这面板的Application）
 * 这一栏可用看到）。向同一个域名下发送请求，都会携带相同的cookie，服务器拿到Cookie进行解析，便能拿到客户端状态
 * 
 * Cookie的作用很好理解，就是用来做状态存储的，但它也是有诸多致密缺陷
 * 
 * 1.容量缺陷。Cookie的体积上限只有4kb，只能用来存储少量的信息
 * 
 * 2.性能缺陷。Cookie紧跟域名，不管域名下面的某一个地址需不需要这个cookie，请求都会携带上完整的Cookie，这样
 * 随着请求数的增多，其实会造成巨大的性能浪费的，因为请求携带了很多不必要的内容。
 * 
 * 3.安全缺陷。由于Cookie以纯文本的形式在浏览器和服务器中传递，很容易被非法用户截获，然后进行一些列的篡改，
 * 在cookie的有效期内重新发送给服务器，这是相当危险的。另外，在httpOnly为false的情况下，Cookie信息能直接
 * 通过js脚本来读取。
 * 
 * 
 * localStorage
 * 
 * 和Cookie异同
 * 
 * localStorage有点跟Cookie一样，就是针对一个域名，即在同一个域名下，会存储相同的一段localStorage
 * 
 * 不过它相对cookie还是有相当多的区别的：
 * 
 * 1.容量。localStorage的容量上限为5M，相比于Cookie的4k大大增加。当然这个5M是针对同一个域名的，因此对于一个域名是持久存储的
 * 
 * 2.只存在客户端，默认参与于服务器端的通信。这样就很好的避免了Cookie带来的性能问题和安全问题
 * 
 * 3.接口封装。通过localStorage暴露在全局，并通过它的setItem和getItem等方法进行操作，非常方便
 * 
 * 操作方式
 * 
 * let obj = { name: 'zhangsan', age: 18 }
 * localStorage.setItem("name", 'zhangsan')
 * localStorage.setItem('info', JSON.stringify(obj))
 * 
 * let name = localStorage.getItem('name')
 * let info = JSON.parse(localStorage.getItem('info'))
 * 
 * 从这里可用看出，loaclStorage其实存储的都是字符串，如果是存储对象需要调用json的stringify方法，并且用JSON.parse来解析成对象。
 * 
 * 应用场景
 * 
 * 利用localStorage的较大容量和持久特性，可用利用localStorage存储一些稳定的资源，比如网站的logo，存储base64格式的图片资源，因此利用localStorage
 * 
 * sessionStorage
 * 特点
 * 
 * sessionStorage以下方面和localStorage一致
 * 
 * ·容量。容量上限也为5M
 * ·只存在客户端，默认不参与于服务端的通信
 * ·接口封装。除了sessionStorange名字有所变化，存储方式、操作方式均和localStorage一样
 * 
 * 但sessionStorage和localStorage有一个本质的区别，那就是前者只是会话级别的存储，并不是持久化存储。会话结束
 * 也就是页面关闭，这部分sessionStorage就不复存在了。
 * 
 * 应用场景
 * 
 * 1.可用用来对表单信息进行维护，将表单信息维护在里面，可用保证页面即使刷新也不会让之前的表单信息丢失。
 * 2.可用用来存储本次浏览记录。如果关闭页面后不需要这些记录，用sessionStorage就再合适不过了。事实上微博就采用
 * 这样的存储方式。
 * 
 * IndexDB
 * 
 * IndexDB是运行在浏览器中的非关系型数据库，本质上是数据库，绝不是和刚才WebStorage的5M一个量级，理论上这个容量是没有上限的
 * 
 * IndexDB的重要特性，除了拥有数据库本身的特性，比如支持事务，存储二级制数据，还有这样一些特性需要注意：
 * 
 * 1.键值对存储。内部采用对象仓库存放数据，在这个对象仓库中数据采用键值对的方式来存储
 * 2.异步操作。数据库的读写属于i/o操作，浏览器中对异步I/O提供了支持
 * 3.受同源策略限制，即无法访问跨域的数据库。
 * 
 * 总结
 * 
 * 浏览器中各种本地存储和缓存技术的发展，给前端应用带来了大量的机会，PWA也正是依托了这些优秀的存储方案才得以发展起来。
 * 重新梳理以下这些本地存储方案
 * 1.Cookie并不适合存储，而且存在非常多的缺陷
 * 2.Web Storage包括localStorage和sessionStorage，默认不会参与和服务器的通信
 * 3.IndexDB为运行在浏览器上的非关系型数据库，为大型数据的存储提供了接口
 * 
 * 说一说从输入URL到页面呈现发生了什么
 * 
 * 这是一个无线难得问题。出这个题目的目的就是为了考察你的web基础深入到什么程度。
 * 
 * 你在浏览器地址栏输入百度的网址
 * https://www.baidu.com
 * 
 * 网络请求
 * 
 * 1。构建请求
 * 
 * 浏览器会构建请求行：
 * //请求方法是GET，路径为根路径，HTTP协议为1.1版本
 * GET / HTTP/1.1
 * 
 * 2。查找强缓存
 * 先检查强缓存，如果命中直接使用，否则进入下一步。
 * 
 * 3。DNS解析
 * 由于我们输入的是域名，而数据包是通过IP地址传给对方的。因此我们需要得到域名对应的IP地址。
 * 这个过程需要依赖一个服务系统，这个系统将域名和IP映射，我们将这个系统叫做DNS域名系统。
 * 具体得到IP的过程就是DNS解析
 * 
 * 当然，值得注意的是，浏览器提供了DNS数据缓存功能。即如果一个域名已经被解析过了，那会吧解析的结果缓存下来，
 * 下次处理直接走缓存，不需要经过DNS解析
 * 
 * 另外，如果不指定端口的话，默认采用对应的IP的80端口
 * 
 * 4。建立TCP连接
 * 这里要提醒一点，Chrome在同一个域名下要求同时最多只能有6个TCP连接，超过6个的话剩下的请求就得等待。
 * 
 * 假设现在不需要等待，我们进入了TCP连接的建立阶段。首先解释以下TCP是什么
 * 
 * TCP:（Transmission Control Protocol, 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议
 * 
 * 建立TCP连接经历了下面三个阶段
 * 
 * 1.通过三次握手（即总共发送3个数据包确认已建立连接）建立客户端和服务端之间的连接。
 * 2.进行数据传输。这里有一个重要的机制，就是接收方接收到的数据包后必须要向发送放确认，如果发送放没有接收到这个
 * 确认的消息，就判定为数据丢包，并重新发送改数据包。当然，发送的过程还有一个优化策略，就是把大的数据包拆成一个个
 * 小包，依次传输到接收方，接收方按照这个小包的顺序把他们组装成完整的数据包
 * 3.断开连接的阶段。数据传输完成，现在要断开连接了，通过四次挥手来断开连接
 * 
 * 5.发送HTTP请求
 * 
 * 现在TCP连接完毕，浏览器可以和服务器开始通信，即开始发送HTTP请求。浏览器发HTTP请求要携带三样东西
 * 请求行、请求头、请求体
 * 
 * 首先，浏览器会向服务器发送请求行，关于请求行，我们在这部分的第一步就构建完成l
 * 
 * GET / HTTP/1.1
 * 
 * 构造很简单，由请求方法、请求URL和HTTP版本协议组成
 * 
 * 同时也要带上请求头，比如之前说的Control-Cache、If-Modified-Since、If-None-Match都有可能
 * 被放进请求头当中作为缓存的标识信息。当然还有一些其他的属性。
 * 
 * 最后是请求体，请求体只有在POST方法下存在，常见的场景是表单提交
 * 
 * 网络响应
 * 
 * HTTP请求到达服务器，服务器进行对应的处理。最后把数据传递给浏览器，也就是返回网络响应
 * 
 * 跟请求部分类似，网络响应具有三个部分：响应行、响应头和响应体
 * 
 * 响应行类似于下面这样
 * 
 * HTTP/1.1 200 ok
 * 
 * 由http协议版本、状态码和状态描述组成
 * 
 * 响应头包含了服务器及其返回数据的一些信息，服务器生成数据的时间、返回的数据类型及对应即将写入的Cookie信息
 * 
 * 响应之后 TCP连接就断开了码
 * 
 * 不一定，这时候要判断connection字段，如果请求头中包含Connection：keep-alive，表示持久连接，这样TCP连接
 * 会一直保持，之后的请求统一站点的资源会复用这个连接
 * 
 * 否则断开TCP连接，请求-响应流程结束。
 * 
 * 总结：
 * 
 * 
 * 从输入URL到页面呈现发生了什么 - 解析算法篇
 * 
 * 完成了网络请求和响应，如果响应头中Content-Type的值是text/html，那么接下来就是浏览器的解析和渲染工作了
 * 
 * 首先来介绍一下解析部分，主要分为以下几个步骤：
 * ·构建DOM树
 * ·样式计算
 * ·生成布局树(Layout Tree)
 * 
 * 构建DOM树
 * 
 * 由于浏览器无法直接理解HTML字符串，因此将这一系列的字节流转换为一种有意义并且方便操作的数据结构，这种
 * 数据结构就是DOM树。DOM树的本质上是一个以Document为根节点的多叉树
 * 
 * 那通过什么样的方式来进行解析呢？
 * 
 * HTML文法的本质
 * 
 * 首先，我们应该清楚把握一点：HTML文法并不是上下文无关文法
 * 
 * 这里有必要讨论以下什么叫做上下文无关文法
 * 
 * 在计算机科学的编译原理学科中，有非常明确的定义
 * 
 * 若一个形式文法G = (N,)
 * 
 * 通俗一点讲，上下文无关文法就是说这个文法中所有产生式的左边都是一个非终结符。
 * 
 * 比如
 * 
 * A -> B
 * 
 * 这个文法中，每个产生式左边都会有一个非终结符，这就是上下文无关的文法。这种情况下xBy一定可以规约出xAy的
 * 
 * 关于它为什么是非上下文无关文法，首先需要大家注意的是，规范的HTML语法，是符合上下文无关文法的，能够体现
 * 它非上下文无关的是不标准的语法。在此我举例一个反例即可证明
 * 
 * 比如解析器扫描到form标签的时候，上下文无关文法的处理方式是直接创建对应form的DOM对象，而真实的HTML5场景中
 * 却不是这样的，解析器会查看form的上下文，如果这个form标签的父标签也是form，那么直接跳过当前的form标签，
 * 否则才创建DOM对象
 * 
 * 常规的编程语言都是上下文无关的，而html却相反，也正是它非上下文无关的特性，决定了HTML parser并不能使用
 * 常规编程语言解析器来完成，需要另辟蹊径
 * 
 * 解析算法
 * 
 * html5规范详细的介绍了解析算法，这个算法分为两个阶段
 * 
 * 1.标记化
 * 2.建树
 * 
 * 对应的两个过程就是词法分析和语法分析
 * 
 * 标记化算法
 * 
 * 这个算法输入为HTML文本，输出为HTML标记，也称为标记生成器。其中运用有限自动状态机来完成。即在当前状态下
 * 接收一个或多个字符，就会更新到下一个状态
 * 
 * <html>
 *  <body>
 *    Hello
 *  </body>
 * </htm>
 * 
 * 通过一个简单的例子来演示一下标记化过程
 * 遇到<,状态标记为打开
 * 接收[a-z]的字符，会进入标记名称状态
 * 这个状态一致保持，直到遇到>，表示标记名称记录完成，这时变为数据状态
 * 接下来遇到body标签做同样的处理
 * 这个时候html和body的标记都记录好了。
 * 现在来到<body>中的>,进入数据状态，之后保持这样的状态接收后面的字符hello
 * 接着接收</body>中的<,回到标记打开，接收下一个/后，这个时候会创建一个end tag的token
 * 随后进入标记名称状态，遇到>回到数据状态
 * 接着以同样的样式处理</body>
 * 
 * 建树算法
 * 
 * 之前提到过，DOM树是一个以document为跟节点的多叉树。因此解析器首先会创建一个document对象。标记生成器会把
 * 每个标记的信息发送给构建树。构建树接收到响应的标记时，会创建响应的DOM对象。创建这个DOM对象后会做两件事：
 * 1.将DOM对象加入DOM树中
 * 2.将对应标记压入存放开放（与闭合标签意思对应）元素的栈中
 * 
 * 还是拿下面这个例子说
 * 
 * <html>
 *  <body>
 *    Hello
 *  </body>
 * </html>
 * 
 * 首先状态为初始状态
 * 
 * 接收到标记生成器传过来的html标签，这时候状态变为before html状态。同时创建一个HTMLhtmlElement的DOM元素，
 * 将其加到document根对象上，并进行压栈操作
 * 
 * 接着状态自动变为before head，此时从标记生成器那边传来body，表示并没有head，这时构建树会自动创建一个
 * HTMLHeadElement并将其加入到DOM树中
 * 
 * 现在进入到in head状态，然后直接跳到after head。
 * 
 * 现在标记生成器传来了body标记，创建HTMLBodyElement，插入到DOM树中，同时压入开放标记栈
 * 
 * 接着状态变为in body，然后来接收后面的一系列的字符：Hello。接收到第一个字符时候，会创建一个Text节点并把
 * 字符插入其中，然后把Text节点插入到DOM树中body元素的下面。随着不断接收后面的字符，，这些字符会附在Text节点上
 * 
 * 现在，标记生成器传过来一个body的结束标记，进入到after body状态
 * 
 * 标记生成器最后传过来一个html结束标记，进入after after body的状态，表示解析过程到此结束。
 * 
 * 容错机制
 * 
 * 教导html5规范，就不得不说它强大的宽容策略，容错能力非常强，虽然大家褒贬不一，有必要知道HTML Parser在容错方面
 * 做了一些事情
 * 
 * 接下来是Webkit中一些经典的容错示例
 * 
 * 1.使用</br>而不是<br>
 * 
 * if(t->isCloseTag(brTag) && m_document->inCompatMode()){
 *  reportError(MalformedError)
 *  t->beginTag = true
 * }
 * 
 * 全部换为<br>的形式
 * 
 * 2.表格离散
 * 
 * <table>
 *  <table>
 *    <tr><td>inner table</td></tr>
 *  </table>
 *  <tr><td>outer table</td></tr>
 * </table>
 * 
 * webkit会自动转换为
 * <table>
 *  <tr><td>outer table</td></tr>
 * </table>
 * <table>
 *  <tr><td>inner table</td></tr>
 * </table>
 * 
 * 样式计算
 * 
 * 关于css样式，它的来源一般是3种
 * 1。link标签引用
 * 2.style标签中的样式
 * 3.元素的内联style样式
 * 
 * 格式化样式表
 * 
 * 首先，浏览器是无法直接识别css样式文本的，因此渲染引擎接收到css文本之后，第一件事情就是将其
 * 转化为一个构建化的对象，即styleSheet
 * 
 * 这个格式化的过程过于复杂，而且对于不同的浏览器会有不同的优化策略
 * 
 * 在浏览器控制台能够通过document.styleSheet来查看这个最终的结构。当然，这个结构包含了以上三种
 * css来源，为后面的样式操作提供了基础
 * 
 * 标准化样式属性
 * 
 * 有一些css样式的数值并不容易被渲染引擎所理解，因此需要在计算样式之前将他们标准化如，
 * em -> px  red -> #ff0000 bold->700等
 * 
 * 计算每个节点的具体样式
 * 
 * 样式已被格式化和标准化，接下来就可以计算每个节点的具体样式信息了
 * 
 * 其实计算的方式并不复杂，主要就是两个规则：继承和层叠。
 * 
 * 每个子节点都会默认继承父节点的样式属性，如果父节点中没有找到，就会采用浏览器的默认样式。也叫userAgent样式。
 * 这就是继承规则，
 * 
 * 然后是层叠规则，css最大的特点在于它的层叠性，也就是最终的样式取决于各个属性共同作用的效果，甚至有很多诡异
 * 的层叠现象，
 * 
 * 不过值得注意的是，计算完样式之后，所有的样式会被挂载到window.getComputedStyle当中，也就是可以通过js来获取计算后的样式，非常方便
 * 
 * 生成布局树
 * 
 * 已经生成了DOM树和DOM样式，接下来要做的就是通过浏览器的布局系统确定元素的位置，也就是要生成一颗布局树（Layout Tree）
 * 
 * 布局树生成的大致工作如下
 * 
 * 1.遍历生成的dom树节点，并把他们添加到布局数中
 * 2.计算布局树节点的坐标位置
 * 
 * 值得注意的是，这个布局树只包含可见元素，对于head标签和设置display:none的元素将不会被放入其中
 * 
 * 有人说首先会生成Render Tree,也就是渲染树，其实这还是16年之前的事情，现在chrome团队已经做了大量的重构，已经
 * 没有生成Render Tree的过程了。而布局树的信息以及非常完善，完全拥有Render Tree的功能
 * 
 * 从输入url到页面呈现发生了什么-渲染过程篇
 * 
 * 上一节介绍了浏览器解析的过程，其中包含构建DOM，样式算和构建布局树
 * 
 * 接下来拆解下一个过程--渲染，分为以下步骤：
 * ··建立图层树（Layout tree）
 * ··生成绘制列表
 * ··生成图块并栅格化
 * ··显示器显示内容
 * 
 * 一建图层树
 * 
 * 如果你觉得现在DOM节点也有了，样式和位置节点也有了，可以开始绘制页面了，那你就错了
 * 
 * 因为你考虑掉了另外一些复杂场景，比如3D动画如何呈现出变换效果，当元素含有层叠上下文时，如何控制显示和隐藏等待
 * 
 * 为了解决如上所述问题，浏览器在构建完布局树之后，还会对特定的节点进行分层，构建一颗图层树（Layout Tree）
 * 
 * 那么这颗图层树是根据什么来布局的呢
 * 
 * 一般情况下，节点的图层会默认属于父亲节点的图层（这些图层也称为合成层）。那什么时候会提升为一个单独的合成层呢？
 * 
 * 有两种情况需要分别讨论， 一种是显示合成，一种是隐式合成
 * 
 * 显示合成
 * 
 * 下面是显示合成的情况：
 * 
 * 一,拥有层叠上下文的节点
 * 
 * 层叠上下文也基本上是有一些特定的css属性创建的，一般有以下情况：
 * 1.HTML根元素节点本身就有层叠上下文
 * 2.普通元素设置position部位static并且设置z-index属性，会产生层叠上下文
 * 3.元素的opacity值不是1
 * 4.元素的transform值不是none
 * 5.元素的filter值不是none
 * 6.元素的isolation值是isolate
 * 7.will-change指定的属性值为上面任意一个
 * 
 * 二需要裁剪的地方
 * 
 * 比如一个div，只设置了100*100大小，放了非常多的文字，超出的文字被裁剪。
 * 当然如果出现滚动条，那么滚动条会被单独提升为一个图层
 * 
 * 隐式合成
 * 
 * 接下来是隐式合成，简单地来说就是层叠等级底的节点被单独提升为一个单独的图层之后，那么所有层叠等级比它高
 * 的节点都会成为一个单独的图层，
 * 
 * 这个隐式合成其实隐藏着巨大的风险，如果在一个大型应用中，当一个z-INDEX比较低的元素被提升为一个单独的图层之后
 * 层叠在它上面的元素统统都会被提升为单独的图层，可能会增加上千个图层，大大增加内存压力，甚至让页面奔溃，着就是
 * 层爆炸的原理
 * 
 * 值得注意的是，当需要re'paint时，只需要repaint本身，而不会影像到其他的层
 * 
 * 二生成绘制列表
 * 
 * 接下来渲染引擎会将图层的绘制拆分成一个个绘制指令，比如先画背景、在描绘边框。。。然后将这些指令按顺序组合成
 * 一个待绘制列表，相当于给后面的绘制操作做了一波计划
 * 
 * 三 生成图块和生成位图
 * 
 * 现在开始绘制操作，实际上在渲染进程中绘制操作是由专门的线程来完成的，这个线程叫合成线程
 * 
 * 绘制列表准备好了之后，渲染进程的主进程会给合成线程发送commit消息，把绘制列表提交给合成线程，接下来就是合成线程一战红土的时候了
 * 
 * 首先考虑到视口就这么大，当页面非常大的时候，要划很长的时间才能划到低，如果要一口气全部绘制出来时相当浪费性能的
 * 因此，合成线程要做的 第一件事情就是将图层分块，这些块的大小
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 
 * 说一说从url到页面呈现发生了什么----渲染过程篇
 * 
 * 上一节介绍了浏览器解析的过程，其中包含了构建DOM，样式计算，和构建布局树
 * 
 * 接下来就拆解下一个过程----渲染。分为以下几个步骤
 * 
 * 1.建立图层树（Layer Tree）
 * 2.生成绘制列表
 * 3.生成图块并栅格化
 * 4，显示器显示内容
 * 
 * 一。建图层树
 * 
 * 如果你也觉得现在DOM节点也有了，样式和位置信息也有了，可以开始绘制页面了，那你就错了。
 * 因为你考虑掉了另外一些复杂场景，比如3D动画如何呈现出变换效果，当元素含有层叠上下文时，
 * 如何控制显示和隐藏等等。
 * 
 * 为了解决如上问题，浏览器在构建完布局树之后，还会对特定节点进行分层，构建一棵图层树（Layer Tree）
 * 那么这颗图层树是根据什么来构建的呢？
 * 
 * 一般情况下，节点的图层会默认属于父亲节点的图层（这些图层也称为合成层）。那什么时候会提升为一个
 * 单独的合成层呢？
 * 
 * 有两种情况要分别讨论，一种是显示合成，一种是隐式合成
 * 
 * 显示合成
 * 下面是显示合成的情况：
 * 一，拥有层叠上下文的节点
 * 层叠上下文也基本上是有一些特定的css属性创建的，一般有以下情况：
 * 1.HTML根元素本身就具有层叠上下文
 * 2.普通元素设置position不为static并且设置了z-index属性，会产生层叠上下文
 * 3.元素的opacity不为1
 * 4.元素的transform值不是none
 * 5.元素的filter值不是none
 * 6.元素的isolation值不是iosloate
 * 7.will-change指定的属性值为上面任意一个。
 * 
 * 二，需要裁剪的地方
 * 比如一个div，你只给他设置了100*100像素的大小，而你在里面放了非常多的文字，那么超出的
 * 文字部分就需要被裁剪。当然如果出现了滚动条，那么滚动条被单独提升为一个独立的图层
 * 
 * 隐式合成
 * 接下来是隐式合成，简单来说就是层叠等级低的节点被提升为单独的图层之后，那么所有层叠顶级比他高
 * 的节点都会成为一个单独的图层
 * 
 * 这个隐式合成其实隐藏着一个巨大的风险，如果一个大型应用中，当一个z-index比较低的元素被提升为单独
 * 图层之后，层叠在他上的元素都会统统被提升为一个单独的图层，可能会增加上千个图层，大大的增加内存的
 * 压力，甚至直接让页面崩溃，这就是层爆炸的原理
 * 
 * 二，生成绘制列表
 * 
 * 接下来渲染引擎会将图层的绘制分为一个个绘制指令，比如先画背景，再描绘边框。。然后将这些指令
 * 组合成为一个待绘制列表，相当于给后面的绘制操作做了一波计划。
 * 
 * 三，生成图块和生成位图
 * 
 * 现在开始绘制操作，实际上在渲染进程的绘制中绘制操作是由专门的线程来操作的，这个线程叫做合成线程
 * 绘制列表准备好了之后，渲染进程的主线程会给合成线程发送commit消息，把绘制列表提交给合成线程。
 * 接下来就是合成线程一展宏图的时候了
 * 
 * 首先，考虑到视口就这么大，当页面非常大的时候，要滑很长的时间才能滑到底，如果要一口气全部绘制出来
 * 是相当浪费性能的。因此，合成线程要做的第一件事情就是将图层分块。这些快的大小一般不会太大，通常
 * 是256*256或者512*512这个规格。这样可以大大 加速页面的首屏展示。
 * 
 * 因为后面的图块数据要进入GPU内存，考虑到浏览器内存上传到GPU内存测操作比较慢，即使是绘制一部分图块
 * 也可能消耗大量的时间。针对这个问题，Chrome采用了一个策略：在首次合成图块时只采用一个低分辨率的图片
 * 这样首屏展示的时候只展示出一个低分辨率的图片，这个时候继续合成操作，当正常的图块内容合成完毕之后
 * 会将低分辨率的图块内容替换。这也是Chrom底层优化首屏加载速度的一个手段
 * 
 * 顺便提醒一点，渲染进程中专门维护一个栅格化线程池，专门负责把图块转换为位图数据
 * 然后合成线程会选择视口附近的图块，把他交给栅格化线程池生成位图。
 * 
 * 生成位图的过程实际上都会使用GPU加速，生成的位图最后发送给合成线程
 * 
 * 四，显示器显示内容
 * 
 * 栅格化操作完成后，合成线程会生成一个绘制命令，即“DrawQuad”,并发送给浏览器进程
 * 
 * 浏览器进程中的viz组件接收到这个命令后，根据这个命令，把页面内容绘制到内存，也就是生成了页面
 * 然后，把这部分内存发送给显卡，为什么要发送给显卡呢？我想有必要先聊一聊显示器显示图像的原理
 * 
 * 无论是pc还是手机屏幕，都有一个固定的刷新频率，一般是60HZ，即60帧，也就是一秒更换60张图片，
 * 一张图片停留的时间约为16.7ms，而每次更新的图片都来自显卡的前缓存区。而显卡接收到浏览器进程
 * 传来的页面后，会合成响应的图像，并将图像保存到后缓冲区，然后系统自动将前缓冲区和后缓冲区
 * 对换位置，如此循环更新
 * 
 * 看道这里，你也就明白了，当某个动画大量占用内存的时候，浏览器生成图像的时候会变慢，图像传送
 * 给显卡就会不及时，而显示器还是以不变的频率刷新，因此会出现卡顿，也就是明显的掉帧现象
 * 
 * 
 * 谈谈你对重绘和回流的理解
 * 
 * 我们首先回顾一下渲染流水线的流程
 *                                                生成
 * 生成DOM树 -> 样式计算 -> 生成布局树 -> 建图层树 -------> 绘制列表
 * 
 *                渲染进程中的主线程中进行
 * 
 * 合成线程中     调用线程池
 * 分图块   ->   生成位图   ->   浏览器进程   ->  显卡缓存  ->  显示器
 * 
 *                 渲染进程的主线程之外
 * 
 * 接下来我们将以此为依据来介绍重回和回流，以及让更新视图的另外一种方式----合成
 * 
 * 回流
 * 
 * 首先介绍回流。回流也叫重排
 * 
 * 触发条件
 * 
 * 简单来说，我们对DOM结构的修改引发DOM几何尺寸发送变换的时候，会发生回流的过程
 * 具体一点，有以下的操作会触发回流
 * 
 * 1.一个DOM元素的几何属性变化，常见的几何属性有width, height, padding, margin, left, top, border
 * 等等，这个很好理解
 * 2.使DOM节点发送增减或移动
 * 3.读写offset族，scroll族和client族的属性的时候，浏览器为了获得这些值，需要进行回流操作
 * 4.调用window.getComputedStyle方法
 * 
 * 回流过程
 * 
 * 依照上面的渲染流水线，触发回流的时候，如果DOM结构发生了改变，则重新渲染DOM树，然后将后面的流程
 * （包括主线程之外的任务）全部走一遍
 * 
 * 若DOM结构改变，需要重新生成DOM树
 * |                                              生成
 * 生成DOM树 -> 计算样式 -> 生成布局树 -> 建图层树 --------> 绘制列表
 *  -------     -------     -------     -------            ------
 *              渲染进程中的主线程中进行
 * 
 * 相当于将解析和合成的过程重新又走了一遍，开销是非常大的
 * 
 * 重绘
 * 
 * 触发条件
 * 当DOM的修改导致了样式的变化，并没有影响几何属性的时候，会导致重绘(repaint)
 * 
 * 重绘过程
 * 
 * 由于没有导致DOM几何属性的变化，因此元素的位置信息不需要更新，从而省去了布局的过程
 * 流程如下
 * 
 * 生成DOM树 -> 计算样式 -> 生成布局树 -> 建图层树 -> 绘制列表
 *              -------                             ------
 * 
 * 跳过了生成布局树和建图层树的阶段，直接生成绘制列表，然后继续进行分块，生成位图等后面一系列操作
 * 可以看到，重绘不一定导致回流，但回流一定会发生重绘
 * 
 * 合成
 * 
 * 还有一种情况，是直接合成。比如利用CSS3的transform、opacity、filter这些属性就可以实现合成效果
 * 也就是大家常说的GPU加速
 * 
 * GPU加速的原因
 * 
 * 在合成的情况下，会直接跳过布局和绘制流程，直接进入非线程处理部分，即直接交给合成线程处理。
 * 交给他处理有两大好处
 * 
 * 1.能够充分发挥GPU的优势，合成线程生成位图的过程会调用线程池，并在其中使用GPU进行加速，而GPU是
 * 擅长处理位图数据的
 * 2.没有占用主线程资源，即使主线程卡住了，效果依然能够流畅的展示
 * 
 * 
 * 实践意义：
 * 
 * 知道上面的原理之后，对开发过程有什么指导意义呢？
 * 1.避免频繁使用style，而是采用修改class的方式
 * 2.使用createDocumentFragment进行批量DOM操作
 * 3.对resize,scroll进行防抖节流处理
 * 4.添加will-change:transform，让渲染引擎为其单独实现一个图层，当这些变化发生时，仅仅利用合成线程
 * 去处理这些变换，而不牵扯到主线程，大大提高了渲染效率。当然这边变化不限于transform，任何可以实现
 * 合成效果的css属性都能用will-change来声明
 * 
 * 能不能说一下XSS攻击
 * 
 * 什么是XSS攻击
 * 
 * XSS全称是 Cross site scripting(即跨站脚本)，为了和css坐区别，固叫做XSS。XSS攻击是指浏览器
 * 中执行恶意脚本（无论跨域还是同域），从而拿到用户的信息进行操作
 * 
 * 这些操作一般可以完成下面这些事情
 * 1.窃取cookie
 * 2.监听用户行为，比如输入账号密码后直接发送到黑客服务器
 * 3.修改DOM伪造表单登录
 * 4.在页面中生成浮窗广告
 * 
 * 通常情况，XSS攻击的实现方式有三种----存储型，反射型和文档型。原理都比较简单
 * 
 * 存储型
 * 
 * 存储型，顾名思义就是将恶意脚本存储起来，缺失，存储型的XSS将脚本存储到了服务端的数据库，然后
 * 在客户端执行这些脚本，从而达到攻击的效果。
 * 
 * 常见的场景是留言评论区提交一段脚本代码，如果前后端没有做好转义工作，那评论内容存到了数据库中
 * 在页面渲染过程中直接执行，相当于执行一段未知逻辑的js代码，是非常恐怖的，这就是XSS存储型攻击
 * 
 * 反射型
 * 
 * 反射型XSS指的是恶意脚本作为网络请求的一部分。
 * 
 * 比如我输入：
 * http://sanyuan.com?q=<script>alert('xxx')</script>
 * 
 * 这样，在服务端会拿到q参数，然后将内容返回给浏览器端，浏览器将这些内容作为html的一部分解析，
 * 发现是一个脚本，直接执行，这样就被攻击了
 * 
 * 之所以叫他反射型，是因为恶意脚本是通过作为网络请求的参数，经过服务器，然后返回到html文档中，执行
 * 和解析。和存储型不一样的是，服务器并不存储这些恶意脚本
 * 
 * 文档型
 * 
 * 文档型XSS攻击并不经过服务器，而是作为中间人的角色，在数据传输过程中劫持到网络数据包，然后修改里面的
 * html文档
 * 这样的劫持方式包括WIFI路由器劫持或者本地恶意软件等。
 * 
 * 防范措施
 * 
 * 明白了三种XSS攻击的原理，我们能发现一个共同点：都是让恶意脚本直接能在浏览器中执行。
 * 那么要防范他，就是要避免这些脚本代码的执行
 * 为了做到这些，必须做到一个信念两个利用
 * 
 * 1.一个信念
 * 千万不要相信任何用户输入
 * 无论在前端和服务器端，都要对用户的输入进行转码和过滤
 * 
 * 如：
 * <script>alert('xxx')</script>
 * 
 * 转码后
 * &lt;script&gt;alert().......
 * 
 * 这样的代码在html解析过程中是无法执行的
 * 当然也可以利用关键词过滤的方式，将script标签删除。那么内容只剩下空
 * 
 * 利用CSP
 * 
 * csp，即浏览器的内容安全策略，他的核心思想就是服务器决定浏览器加载哪些资源， 具体来说可以完成以下功能
 * 1.限制其他域下的资源加载
 * 2.禁止向其他域提交数据
 * 3.提供上报机制，帮我们发现XSS攻击
 * 
 * 利用httpOnly
 * 很多xss攻击脚本都是用来窃取Cookie，而设置Cookie的httpOnly属性后，js便无法读取cookie值，这样也能
 * 很好的防范XSS攻击
 * 
 * 总结：
 * XSS攻击是指浏览器中执行恶意脚本，然后拿到用户信息进行操作。主要分为存储型，反射型和文档型。防范的措施包括
 * 1.一个信念：不要相信用户的输入，对输入内容进行转码和过滤，让其不可执行
 * 2.两个利用：利用csp，利用cookie的httpOnly属性
 * 
 * 能不能说一下CSRF攻击
 * 
 * 什么是CSRF攻击
 * 
 * CSRF（cross-site request forgery），即跨站请求伪造，指的是黑客诱导用户点击连接，打开黑客的网站
 * 然后黑客利用用户目前的登录状态发起跨站请求。
 * 
 * 举个例子，你在某个论坛点击了黑客精心挑选的小姐姐图片，你点击进去，进入了一个新的页面，那么恭喜你你被攻击了
 * 
 * 你可能会比较好奇，怎么突然就被攻击了呢？接下来我们就拆解以下当你点击了连接之后，黑客在背后做了哪些事情
 * 
 * 可能会做三样事情。举例如下
 * 
 * 1.自动发GET请求
 * 
 * 黑客页面里可能有一段这样的代码
 * <img src="https://xxx.com/info?user=hhh&count=100"/>
 * 
 * 进入页面后会自动发送get请求，值得注意的是，这个请求会自动带上关于xxx.com的cookie信息
 * (这里假定你已经在xxx.com中登录过了)
 * 
 * 加入服务端没有相应的验证机制，他可能会任务发请求的是一个正常的用户，应为携带了相应的cookie
 * 然后进行相应的各种操作，可以是转账汇款以及其他恶意操作。
 * 
 * 2.自动发post请求
 * 
 * 黑客可能自己填了一个表单，写了一段自动提交的脚本
 * 
 * <form action="https://xxx.com/info" method="post">
 *    <input type="hidden" name="user" value="hhh"/>
 *    <input type="hidden" name="count" value="100"/>
 * </form>
 * <script>dom.submit()</script>
 * 
 * 同样也会携带相应的用户cookie信息，让服务器以为是一个正常的用户操作，让各种恶意操作变为可能
 * 
 * 诱导点击发送GEt请求
 * 
 * 在黑客的网站上，可能会放一个链接，驱使你来点击
 * 
 * <a href="http://xxx/info?user=hhh&count=100" target="_blank">点击</a> 
 * 
 * 点击后，自动发送get请求，接下来和自动发送get请求部分同理
 * 
 * 这就是CSRF攻击的原理。和xss相比，csrf攻击并不需要将恶意代码注入用户当前页面的html文档中，
 * 而是跳转到新的页面，利用服务器的验证漏洞和用户之前的登录状态来模拟用户进行操作
 * 
 * 防范措施
 * 
 * 1.利用cookie的SameSite属性
 * 
 * CSRF攻击中重要的一环就是自动发送目标站点下的cookie，然后就是这一份cookie模拟了用户的身份。
 * 因此cookie上面下文章是防范的不二之选
 * 
 * 恰好，在cookie当中有一个关键的字段，可以对请求中cookie的携带做一些限制操作，这个字段就是sameSite
 * samesite可以设置三个值，strict、lax和None
 * 
 * a。在strict模式下，浏览器完全禁止第三方请求携带cookie。比如请求sanyuan.com网站只能在sanyuan.com
 * 域名中请求才能携带cookie，其他网站请求都不能
 * b。在Lax模式下，就宽松了一点，但只能在get方法提交表单或者a标签发送请求的情况下携带cookie，其他情况均不能
 * c。在None模式下，也就是默认模式，请求会自动携带上cookie
 * 
 * 2.验证来源站点
 * 
 * 这就需要用到请求头中的两个字段：Origin和Referer
 * 其中，Origin只包含域名信息，而Referer包含了具体的URL路径
 * 当然这两者都是可以伪造的，通过ajax中自定义请求头即可，安全性略差
 * 
 * 3.CSRF Token
 * 
 * Django作为Python的一门后端框架，如果是用它开发过的同学就知道，在他的模板中开发表单时
 * 经常会附上这样一行代码
 * {% csrf_token %}
 * 
 * 这就是ccsrf token的典型应用。那他的原理是怎样的呢？
 * 
 * 首先，浏览器向服务器发送请求时，服务器生成一个字符串，将其植入到返回的页面中。
 * 然后浏览器如果要发送请求，就必须带上这个字符串，然后服务器来验证是否合法，如果不合法则不予相应
 * 这个字符串也就时csrf token，通常第三方网站无法拿到这个token因此也就是被服务器给拒绝了。
 * 
 * 总结
 * 
 * CSRF（Cross-site request forgery）即跨站请求伪造，指的是黑客诱导用户点击链接，打开黑客的网站
 * 然后黑客利用用户目前的登录状态发起跨站请求
 * 
 * csrf攻击一般会有三种方式
 * 1.自动get请求
 * 2.自动post请求
 * 3.诱导点击发送get请求
 * 
 * 防范措施：利用cookie的sameSite属性、验证来源站点和csrf token
 * 
 * 
 * https为什么让数据传输更安全
 * 
 * 谈到https，就不得不谈到与之相对的http。http的特性是明文传输，因此在传输的每一个环节，数据都有可能
 * 被第三方窃取和篡改，具体来说，http数据经过TCP层，然后经过WIFI路由器、运营商和目标服务器，这些环节
 * 中都可能被中间人拿到数据并篡改，也就是我们常说的中间人
 * 
 * 为了防范这样一类攻击，我们不得已要引入新的加密方案，即https
 * 
 * https并不是一个新的协议，而是一个加强版的http。其原理是在http和tcp之间建立了一个中间层，当
 * http和TCP通信时并不是像以前那也直接通信，直接经过了一个中间层进行加密，将加密后的数据包传递给
 * tcp，响应的，tcp必须将数据包解密，才能传给上面的http。这个中间层叫做安全层。安全层的核心就是
 * 对数据进行加解密。
 * 
 * 接下来我们就剖析以下https的加解密时如何实现的
 * 
 * 对称加密和非对称加密
 * 
 * 概念
 * 
 * 首先需要理解对称加密和非对称加密的概念，然后 讨论两者应用后的效果如何
 * 
 * 对称加密时最简单的方式，指的是加密和解密用的是同样的密钥
 * 
 * 而对于非对称加密，如果有a、b两把密钥，如果用a加密过的数据包只能用b解密，反之，如果用b加密的数据包
 * 只能用a解密
 * 
 * 加解密过程
 * 
 * 接着我们来谈谈浏览器和服务器进行协商加密的过程
 * 首先，浏览器会给服务器发送一个随机数client_random和一个加密的方法列表
 * 服务器接收后给浏览器返回另一个随机数server_random和加密方法
 * 
 * 现在两者拥有三样相同的凭证：client_random server_random和加密方法
 * 
 * 接着用这个加密方法将两个随机数混合起来生成密钥，这个密钥就是浏览器和服务器通信的暗号
 * 
 * 各自的效果
 * 
 * 如果用对称加密的方式，那么第三方可以在中间获取到client_random server_random和加密方法
 * 由于这个加密方法同时可以解密，所以中间人可以成功对暗号进行机密，拿到数据，很容易就将这种
 * 加密方式破解了
 * 
 * 既然对称加密这么不堪一击，我们就来试一试非对称加密。在这种加密方式中，服务器手里有两把钥匙
 * 一把是公钥，也就是每个人都可以拿到，是 公开的，另一把是私钥，只有服务器自己知道
 * 
 * 好，现在开始传输。
 * 
 * 浏览器把client_random和加密列表传过来，服务器接收到，把server_random、加密方法和公钥传给浏览器
 * 现在两者拥有相同的client_random server_random和加密方法。然后浏览器用公钥将client_random和
 * server_random加密，生成与服务器通信的暗号。
 * 
 * 这时候由于是非对称加密，公钥加密过的数据只能用私钥解密，因此中间人就算拿到浏览器传来的数据，由于
 * 它没有私钥，照样无法解密，保证了数据的安全性
 * 
 * 难道这一定就安全码？回到非对称加密的定义，公钥加密的数据可以用私钥解密，那私钥加密的数据也可以用
 * 公钥进行机密呀
 * 
 * 服务器的数据只能用私钥进行加密（因为如果它用公钥那么浏览器也就没有办法解密了）中间人一旦拿到公钥
 * 那么就可以对服务端传来的数据进行解密，就这样又能破解了，而且，只采用非对称加密，对于服务器的性能
 * 的消耗也是巨大的。因此我们暂不考虑这种方案
 * 
 * 
 * 对称加密和非对称加密结合
 * 
 * 可以发现，对称加密和非对称加密，单独任何一个，都会存在安全隐患。那我们能不能把两者进行结合，进一步
 * 保证安全呢？
 * 
 * 其实是可以的，演示以下整个流程：
 * 
 * 1.浏览器向服务器发送client_random和加密方法列表
 * 2.服务器收到，返回server_random、加密方法以及公钥
 * 3.浏览器接收，接着生成另一个随机数pre_random，并且用公钥加密传给服务器
 * 4.服务器用私钥解密这个被加密后的pre_random
 * 
 * 现在浏览器和服务器有三样相同的凭证：client_random\ server_random pre_random。
 * 然后两者用相同的加密方法混合着三个随机数，生成最终密钥
 * 
 * 然后浏览器和服务器尽管用一样的密钥进行通信，即使用对称加密
 * 这个最终的密钥是很难被中间人拿到的，为社么呢？因为中间人没有私钥，从而拿不到pre_random，也就无法
 * 生成最终密钥了
 * 
 * 回头比较以下和单纯的使用非对称加密,这两种方式做了什么改进呢?本质上是防止了私钥加密的数据外传
 * 单独使用非对称加密，最大的漏洞在于服务器传数据给浏览器只能用私钥加密，这是危险产出的根源
 * 利用对称和非对称加密结合方式，就防止了这一点，从而保证了安全
 * 
 * 添加数字证书
 * 
 * 尽管通过两种加密方式的结合，能够很好的实现加密传输，但实际上还存在一些问题。黑客如果dns劫持
 * 将目标地址缓存黑客的服务器地址，然后黑客自己造一份公钥和私钥照样能进行数据传输。而对于浏览器
 * 用户而言，他不知道自己正在访问一个危险的服务器
 * 
 * 事实上https在上述结合对称和非对称加密的基础上，又添加了数字证书认证的步骤，其目的就是让服务器
 * 证明自己的身份
 * 
 * 传输过程
 * 
 * 为了获取这个证书，服务器运营着需要向第三方认证机构收取授权，这个第三方机构也叫CA（Certificate Authority）
 * 认证通过后ca会给服务器颁发数字证书
 * 
 * 这个数字证书又两个作用
 * 1.服务器向浏览器证明自己的省份
 * 2.把公钥传给浏览器
 * 
 * 这个验证过程发生在什么时候呢？
 * 
 * 当服务器传输server_random\加密方法 的时候，顺便会带上数字证书(包含了公钥)，接着浏览器接收之后
 * 就会开始验证数字证书。如果验证通过，那么后面的过程照常执行，否则拒绝执行
 * 
 * 现在我们来梳理一些https最终加解密过程：
 * 
 * 浏览器                   服务器
 *   -> client_rendom 加密方法列表
 *   <- server_random 加密方法和数字证书
 * 验证数字证书
 * 验证通过 生成pre_random()  并且server_random client_random pre_random联合生成secret
 * 
 *   -> 公钥加密pre_random
 *                          私钥解密，获得pre_random
 *                          server_random client_random pre_random联合生成secret
 *   <- 服务器端确认
 * 
 *   <--->之后的操作都使用secret作为密钥进行加解密操作。
 * 
 * 
 * 认证过程
 * 
 * 浏览器拿到数字证书后，如何来对证书进行认证呢？
 * 
 * 首先，会读取证书中的明文内容。ca进行数字证书的签名时会保存一个hash函数，用这个函数计算明文
 * 内容得到信息A，然后用公钥解密明文内容得到信息B，两份信息坐对比，一致则表示认证合法
 * 
 * 当然有时候对于浏览器而言，他不知道哪些ca是值得信任的，因此会继续查找ca的上级ca，以相同的
 * 信息对比方式验证上级ca的合法性。一般根级的ca会内置造操作系统当中，当然如果向上没有找到上级
 * ca，那么将被视为不合法
 * 
 * 总结
 * 
 * HTTPS并不是一个新的协议，它在http和tcp的传输中建立了一个安全层，利用对称加密和非对称加密
 * 结合数字证书认证的方式，让传输过程的安全性大大提高
 * 
 * 
 * 
 * 
 */